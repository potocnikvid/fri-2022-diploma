{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Child to Parents mappings in StyleGAN2 latent space using the Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vidp/miniconda3/envs/diploma_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vidp/miniconda3/envs/diploma_env/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/vidp/miniconda3/envs/diploma_env/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefIlEENS2_8optionalINS2_10ScalarTypeEEENS5_INS2_6LayoutEEENS5_INS2_6DeviceEEENS5_IbEENS5_INS2_12MemoryFormatEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from dataset.nok_mean import NokMeanDataset\n",
    "from dataset.nok_mean_real_c2p import NokMeanRealDatasetC2P\n",
    "from dataset.nok import NokDataset\n",
    "from dataset.nok_aug import NokAugDataset\n",
    "from utils.stylegan import StyleGAN2\n",
    "from utils.eval import BaseEvaluator\n",
    "from utils.viz import image_add_label\n",
    "import utils.nokdb as nokdb\n",
    "import utils.ppldb as ppldb\n",
    "import os\n",
    "from fnmatch import fnmatch\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "root = os.getenv('ROOT')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[1]:\n",
    "Create a ooutput directory for the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./src/.tmp/stylegan2-ada-pytorch/child_to_parent025'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = \"child_to_parent\"\n",
    "output_path = \"./src/.tmp/stylegan2-ada-pytorch/\" + name + \"001\"\n",
    "while (os.path.exists(output_path)):\n",
    "  output_path = output_path[:-3] + str(int(output_path[-3:]) + 1).zfill(3)\n",
    "\n",
    "os.mkdir(output_path)\n",
    "output_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[2]:\n",
    "Align faces in ppldb dataset if not already done and save the aligned images in a new directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping ./src/dataset/ppldb/people/nina/ppldb_nina_1.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/nina/ppldb_nina_2.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/nina/ppldb_nina_3.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/nina/ppldb_nina_4.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/nina/ppldb_nina_5.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/nina/ppldb_nina_6.jpg - image already aligned.\n",
      "No images to align\n",
      "Skipping ./src/dataset/ppldb/people/ati/ppldb_ati_1.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/ati/ppldb_ati_4.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/ati/ppldb_ati_3.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/ati/ppldb_ati_2.jpg - image already aligned.\n",
      "No images to align\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_18.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_3.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_2.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_7.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_12.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_5.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_14.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_13.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_6.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_24.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_8.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_9.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_21.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_10.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_15.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_20.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_27.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_17.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_16.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_22.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_26.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_25.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/stef/ppldb_stef_23.jpg - image already aligned.\n",
      "1\n",
      "Running on 4 paths\n",
      "Here we goooo\n",
      "\tForkPoolWorker-1 is starting to extract on #4 images\n",
      "[[[129 132 137]\n",
      "  [137 140 145]\n",
      "  [129 132 137]\n",
      "  ...\n",
      "  [ 87  89 101]\n",
      "  [ 88  90 102]\n",
      "  [ 97  99 111]]\n",
      "\n",
      " [[135 138 143]\n",
      "  [136 139 144]\n",
      "  [128 131 136]\n",
      "  ...\n",
      "  [ 81  83  95]\n",
      "  [ 88  90 102]\n",
      "  [ 94  96 108]]\n",
      "\n",
      " [[129 132 137]\n",
      "  [136 139 144]\n",
      "  [133 136 141]\n",
      "  ...\n",
      "  [ 92  94 106]\n",
      "  [ 83  85  97]\n",
      "  [ 85  87  99]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 84  84  84]\n",
      "  [ 90  90  90]\n",
      "  [ 90  90  90]\n",
      "  ...\n",
      "  [ 89  91 103]\n",
      "  [ 81  84  93]\n",
      "  [ 86  89  98]]\n",
      "\n",
      " [[ 87  87  87]\n",
      "  [ 90  90  90]\n",
      "  [ 89  89  89]\n",
      "  ...\n",
      "  [ 77  79  91]\n",
      "  [ 86  89  98]\n",
      "  [ 90  93 102]]\n",
      "\n",
      " [[ 88  88  88]\n",
      "  [ 95  95  95]\n",
      "  [ 90  90  90]\n",
      "  ...\n",
      "  [ 87  89 101]\n",
      "  [ 82  85  94]\n",
      "  [ 90  93 102]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/stef/ppldb_stef_1.jpg - local variable 'shape' referenced before assignment\n",
      "[[[126 126 126]\n",
      "  [153 153 153]\n",
      "  [127 127 127]\n",
      "  ...\n",
      "  [212 177 147]\n",
      "  [213 178 148]\n",
      "  [214 179 149]]\n",
      "\n",
      " [[132 132 132]\n",
      "  [129 129 129]\n",
      "  [136 136 136]\n",
      "  ...\n",
      "  [213 178 148]\n",
      "  [213 178 148]\n",
      "  [213 178 148]]\n",
      "\n",
      " [[133 133 133]\n",
      "  [119 119 119]\n",
      "  [138 138 138]\n",
      "  ...\n",
      "  [215 178 149]\n",
      "  [215 178 149]\n",
      "  [214 177 148]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[117 117 117]\n",
      "  [115 115 115]\n",
      "  [116 116 116]\n",
      "  ...\n",
      "  [ 79  63  48]\n",
      "  [ 66  50  35]\n",
      "  [ 69  53  38]]\n",
      "\n",
      " [[111 111 111]\n",
      "  [104 104 104]\n",
      "  [116 116 116]\n",
      "  ...\n",
      "  [ 77  61  46]\n",
      "  [ 73  57  42]\n",
      "  [ 71  55  40]]\n",
      "\n",
      " [[122 122 122]\n",
      "  [108 108 108]\n",
      "  [116 116 116]\n",
      "  ...\n",
      "  [ 68  52  37]\n",
      "  [ 65  49  34]\n",
      "  [ 76  60  45]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/stef/ppldb_stef_4.jpg - local variable 'shape' referenced before assignment\n",
      "[[[56 22 20]\n",
      "  [56 22 20]\n",
      "  [60 26 24]\n",
      "  ...\n",
      "  [30 31 33]\n",
      "  [28 29 31]\n",
      "  [31 32 34]]\n",
      "\n",
      " [[59 25 23]\n",
      "  [58 24 22]\n",
      "  [60 26 24]\n",
      "  ...\n",
      "  [27 28 30]\n",
      "  [25 26 28]\n",
      "  [28 29 31]]\n",
      "\n",
      " [[57 23 21]\n",
      "  [59 25 23]\n",
      "  [62 28 26]\n",
      "  ...\n",
      "  [24 25 27]\n",
      "  [23 24 26]\n",
      "  [25 26 28]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[69 34 30]\n",
      "  [68 33 29]\n",
      "  [69 34 30]\n",
      "  ...\n",
      "  [33 34 36]\n",
      "  [38 39 41]\n",
      "  [30 31 33]]\n",
      "\n",
      " [[66 33 28]\n",
      "  [67 34 29]\n",
      "  [68 35 30]\n",
      "  ...\n",
      "  [32 33 35]\n",
      "  [39 40 42]\n",
      "  [31 32 34]]\n",
      "\n",
      " [[64 30 28]\n",
      "  [66 32 30]\n",
      "  [69 35 33]\n",
      "  ...\n",
      "  [34 35 37]\n",
      "  [38 39 41]\n",
      "  [26 27 29]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/stef/ppldb_stef_19.jpg - local variable 'shape' referenced before assignment\n",
      "[[[180 177 170]\n",
      "  [176 173 166]\n",
      "  [167 164 157]\n",
      "  ...\n",
      "  [ 27  27  27]\n",
      "  [ 10  10  10]\n",
      "  [ 10  10  10]]\n",
      "\n",
      " [[178 175 168]\n",
      "  [171 168 161]\n",
      "  [165 162 155]\n",
      "  ...\n",
      "  [ 38  38  38]\n",
      "  [  2   2   2]\n",
      "  [  3   3   3]]\n",
      "\n",
      " [[182 179 172]\n",
      "  [171 168 161]\n",
      "  [167 164 157]\n",
      "  ...\n",
      "  [  9   9   9]\n",
      "  [  6   6   6]\n",
      "  [ 18  18  18]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[206 199 183]\n",
      "  [206 199 183]\n",
      "  [208 201 185]\n",
      "  ...\n",
      "  [ 45  45  57]\n",
      "  [ 48  48  60]\n",
      "  [ 56  56  68]]\n",
      "\n",
      " [[207 200 184]\n",
      "  [207 200 184]\n",
      "  [208 201 185]\n",
      "  ...\n",
      "  [ 49  49  61]\n",
      "  [ 53  53  65]\n",
      "  [ 54  54  66]]\n",
      "\n",
      " [[210 203 187]\n",
      "  [210 203 187]\n",
      "  [209 202 186]\n",
      "  ...\n",
      "  [ 53  53  65]\n",
      "  [ 54  54  66]\n",
      "  [ 41  41  53]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/stef/ppldb_stef_11.jpg - local variable 'shape' referenced before assignment\n",
      "\tDone!\n",
      "Mischief managed in 15.913749694824219s\n",
      "Skipping ./src/dataset/ppldb/people/vid/ppldb_vid_3.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/vid/ppldb_vid_5.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/vid/ppldb_vid_1.jpg - image already aligned.\n",
      "Skipping ./src/dataset/ppldb/people/vid/ppldb_vid_2.jpg - image already aligned.\n",
      "1\n",
      "Running on 10 paths\n",
      "Here we goooo\n",
      "\tForkPoolWorker-1 is starting to extract on #10 images\n",
      "[[[165 140 109]\n",
      "  [166 141 110]\n",
      "  [167 142 111]\n",
      "  ...\n",
      "  [ 71  70  86]\n",
      "  [ 61  60  76]\n",
      "  [ 56  55  71]]\n",
      "\n",
      " [[167 142 111]\n",
      "  [169 144 113]\n",
      "  [170 145 114]\n",
      "  ...\n",
      "  [ 65  64  80]\n",
      "  [ 58  57  73]\n",
      "  [ 56  55  71]]\n",
      "\n",
      " [[167 142 111]\n",
      "  [171 146 115]\n",
      "  [175 150 119]\n",
      "  ...\n",
      "  [ 68  67  83]\n",
      "  [ 63  62  78]\n",
      "  [ 58  57  73]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[169 149 116]\n",
      "  [169 149 116]\n",
      "  [170 150 117]\n",
      "  ...\n",
      "  [ 91  87  88]\n",
      "  [ 91  87  88]\n",
      "  [ 95  91  92]]\n",
      "\n",
      " [[168 148 115]\n",
      "  [169 149 116]\n",
      "  [170 150 117]\n",
      "  ...\n",
      "  [ 91  87  88]\n",
      "  [ 93  89  90]\n",
      "  [ 93  89  90]]\n",
      "\n",
      " [[166 146 113]\n",
      "  [167 147 114]\n",
      "  [169 149 116]\n",
      "  ...\n",
      "  [100  96  97]\n",
      "  [101  97  98]\n",
      "  [ 89  85  86]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_9.jpg - local variable 'shape' referenced before assignment\n",
      "[[[147 127 102]\n",
      "  [179 159 134]\n",
      "  [182 162 137]\n",
      "  ...\n",
      "  [140 152 138]\n",
      "  [151 163 149]\n",
      "  [165 177 163]]\n",
      "\n",
      " [[181 161 136]\n",
      "  [181 161 136]\n",
      "  [186 166 141]\n",
      "  ...\n",
      "  [142 154 140]\n",
      "  [153 165 151]\n",
      "  [165 177 163]]\n",
      "\n",
      " [[180 160 135]\n",
      "  [187 167 142]\n",
      "  [181 161 136]\n",
      "  ...\n",
      "  [149 161 147]\n",
      "  [147 159 145]\n",
      "  [155 167 153]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[128 104  78]\n",
      "  [127 103  77]\n",
      "  [126 102  76]\n",
      "  ...\n",
      "  [ 23  36  29]\n",
      "  [ 64  77  70]\n",
      "  [ 38  51  44]]\n",
      "\n",
      " [[131 107  81]\n",
      "  [128 104  78]\n",
      "  [127 103  77]\n",
      "  ...\n",
      "  [ 71  84  77]\n",
      "  [ 69  82  75]\n",
      "  [ 63  76  69]]\n",
      "\n",
      " [[126 102  76]\n",
      "  [132 108  82]\n",
      "  [138 114  88]\n",
      "  ...\n",
      "  [ 90 103  96]\n",
      "  [ 92 105  98]\n",
      "  [103 116 109]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_11.jpg - local variable 'shape' referenced before assignment\n",
      "[[[ 95  49  26]\n",
      "  [ 95  49  26]\n",
      "  [ 96  50  27]\n",
      "  ...\n",
      "  [ 55  53  56]\n",
      "  [ 59  57  60]\n",
      "  [ 45  43  46]]\n",
      "\n",
      " [[103  57  34]\n",
      "  [103  57  34]\n",
      "  [103  57  34]\n",
      "  ...\n",
      "  [ 59  57  60]\n",
      "  [ 59  57  60]\n",
      "  [ 51  49  52]]\n",
      "\n",
      " [[102  56  33]\n",
      "  [104  58  35]\n",
      "  [107  61  38]\n",
      "  ...\n",
      "  [ 55  53  56]\n",
      "  [ 51  49  52]\n",
      "  [ 58  56  59]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[179 159 124]\n",
      "  [180 160 125]\n",
      "  [180 160 125]\n",
      "  ...\n",
      "  [ 96  95 100]\n",
      "  [ 95  94  99]\n",
      "  [ 89  88  93]]\n",
      "\n",
      " [[179 159 124]\n",
      "  [180 160 125]\n",
      "  [180 160 125]\n",
      "  ...\n",
      "  [ 96  95 100]\n",
      "  [ 90  89  94]\n",
      "  [ 83  82  87]]\n",
      "\n",
      " [[179 159 124]\n",
      "  [180 160 125]\n",
      "  [180 160 125]\n",
      "  ...\n",
      "  [ 93  92  97]\n",
      "  [ 99  98 103]\n",
      "  [104 103 108]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_8.jpg - local variable 'shape' referenced before assignment\n",
      "[[[131 120 100]\n",
      "  [175 164 144]\n",
      "  [162 151 131]\n",
      "  ...\n",
      "  [185 208 192]\n",
      "  [175 198 182]\n",
      "  [172 195 179]]\n",
      "\n",
      " [[167 156 136]\n",
      "  [160 149 129]\n",
      "  [149 138 118]\n",
      "  ...\n",
      "  [174 197 181]\n",
      "  [174 197 181]\n",
      "  [179 202 186]]\n",
      "\n",
      " [[170 157 138]\n",
      "  [149 136 117]\n",
      "  [141 128 109]\n",
      "  ...\n",
      "  [171 194 178]\n",
      "  [176 199 183]\n",
      "  [185 208 192]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[131 105  78]\n",
      "  [137 111  84]\n",
      "  [126 100  73]\n",
      "  ...\n",
      "  [147 169 157]\n",
      "  [149 171 159]\n",
      "  [161 183 171]]\n",
      "\n",
      " [[116  90  63]\n",
      "  [114  88  61]\n",
      "  [105  79  52]\n",
      "  ...\n",
      "  [147 169 157]\n",
      "  [156 178 166]\n",
      "  [152 174 162]]\n",
      "\n",
      " [[113  87  60]\n",
      "  [121  95  68]\n",
      "  [120  94  67]\n",
      "  ...\n",
      "  [157 179 167]\n",
      "  [148 170 158]\n",
      "  [137 159 147]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_14.jpg - local variable 'shape' referenced before assignment\n",
      "[[[126 109  91]\n",
      "  [152 135 117]\n",
      "  [152 135 117]\n",
      "  ...\n",
      "  [184 206 194]\n",
      "  [184 206 194]\n",
      "  [171 193 181]]\n",
      "\n",
      " [[140 123 105]\n",
      "  [165 148 130]\n",
      "  [155 138 120]\n",
      "  ...\n",
      "  [187 209 197]\n",
      "  [200 222 210]\n",
      "  [184 206 194]]\n",
      "\n",
      " [[151 134 116]\n",
      "  [148 131 113]\n",
      "  [150 133 115]\n",
      "  ...\n",
      "  [185 207 195]\n",
      "  [200 222 210]\n",
      "  [202 224 212]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[128 106  82]\n",
      "  [ 98  76  52]\n",
      "  [125 103  79]\n",
      "  ...\n",
      "  [151 171 159]\n",
      "  [149 169 157]\n",
      "  [133 153 141]]\n",
      "\n",
      " [[122 100  76]\n",
      "  [112  90  66]\n",
      "  [134 112  88]\n",
      "  ...\n",
      "  [129 149 137]\n",
      "  [125 145 133]\n",
      "  [125 145 133]]\n",
      "\n",
      " [[108  86  62]\n",
      "  [129 107  83]\n",
      "  [124 102  78]\n",
      "  ...\n",
      "  [161 181 169]\n",
      "  [148 168 156]\n",
      "  [136 156 144]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_12.jpg - local variable 'shape' referenced before assignment\n",
      "[[[160 137 105]\n",
      "  [161 138 106]\n",
      "  [161 138 106]\n",
      "  ...\n",
      "  [ 83  94 100]\n",
      "  [ 79  90  96]\n",
      "  [ 71  82  88]]\n",
      "\n",
      " [[162 139 107]\n",
      "  [163 140 108]\n",
      "  [163 140 108]\n",
      "  ...\n",
      "  [ 81  92  98]\n",
      "  [ 80  91  97]\n",
      "  [ 67  78  84]]\n",
      "\n",
      " [[160 137 105]\n",
      "  [164 141 109]\n",
      "  [163 140 108]\n",
      "  ...\n",
      "  [ 80  91  97]\n",
      "  [ 76  87  93]\n",
      "  [ 66  77  83]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[159 140 108]\n",
      "  [158 139 107]\n",
      "  [157 138 106]\n",
      "  ...\n",
      "  [ 80  82  81]\n",
      "  [ 85  87  86]\n",
      "  [ 80  82  81]]\n",
      "\n",
      " [[159 140 108]\n",
      "  [158 139 107]\n",
      "  [157 138 106]\n",
      "  ...\n",
      "  [ 84  86  85]\n",
      "  [ 85  87  86]\n",
      "  [ 75  77  76]]\n",
      "\n",
      " [[159 140 108]\n",
      "  [158 139 107]\n",
      "  [157 138 106]\n",
      "  ...\n",
      "  [ 83  85  84]\n",
      "  [ 84  86  85]\n",
      "  [ 80  82  81]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_7.jpg - local variable 'shape' referenced before assignment\n",
      "[[[210 210 198]\n",
      "  [206 206 194]\n",
      "  [205 205 193]\n",
      "  ...\n",
      "  [ 71  70  50]\n",
      "  [ 59  58  37]\n",
      "  [ 45  44  23]]\n",
      "\n",
      " [[210 210 198]\n",
      "  [207 207 195]\n",
      "  [205 205 193]\n",
      "  ...\n",
      "  [ 62  61  41]\n",
      "  [ 49  48  27]\n",
      "  [ 36  35  14]]\n",
      "\n",
      " [[210 210 198]\n",
      "  [207 207 195]\n",
      "  [206 206 194]\n",
      "  ...\n",
      "  [ 55  54  34]\n",
      "  [ 42  41  20]\n",
      "  [ 32  31  10]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[180 183 174]\n",
      "  [180 183 174]\n",
      "  [180 183 174]\n",
      "  ...\n",
      "  [171 169 156]\n",
      "  [214 212 199]\n",
      "  [246 247 233]]\n",
      "\n",
      " [[181 184 175]\n",
      "  [181 184 175]\n",
      "  [180 183 174]\n",
      "  ...\n",
      "  [165 163 150]\n",
      "  [204 202 189]\n",
      "  [236 237 223]]\n",
      "\n",
      " [[181 184 173]\n",
      "  [180 183 172]\n",
      "  [180 183 172]\n",
      "  ...\n",
      "  [161 159 146]\n",
      "  [200 198 185]\n",
      "  [233 234 220]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_4.jpg - local variable 'shape' referenced before assignment\n",
      "[[[140 127 111]\n",
      "  [155 142 126]\n",
      "  [149 136 120]\n",
      "  ...\n",
      "  [161 178 162]\n",
      "  [165 182 166]\n",
      "  [163 180 164]]\n",
      "\n",
      " [[145 132 116]\n",
      "  [147 134 118]\n",
      "  [154 141 125]\n",
      "  ...\n",
      "  [128 145 129]\n",
      "  [151 168 152]\n",
      "  [155 172 156]]\n",
      "\n",
      " [[157 144 128]\n",
      "  [154 141 125]\n",
      "  [148 135 119]\n",
      "  ...\n",
      "  [141 158 142]\n",
      "  [146 163 147]\n",
      "  [154 171 155]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[128 104  80]\n",
      "  [124 100  76]\n",
      "  [106  82  58]\n",
      "  ...\n",
      "  [199 217 203]\n",
      "  [153 171 157]\n",
      "  [161 179 165]]\n",
      "\n",
      " [[113  89  65]\n",
      "  [119  95  71]\n",
      "  [112  88  64]\n",
      "  ...\n",
      "  [152 170 156]\n",
      "  [163 181 167]\n",
      "  [164 182 168]]\n",
      "\n",
      " [[127 103  79]\n",
      "  [131 107  83]\n",
      "  [108  84  60]\n",
      "  ...\n",
      "  [134 152 138]\n",
      "  [167 185 171]\n",
      "  [175 193 179]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_13.jpg - local variable 'shape' referenced before assignment\n",
      "[[[160 213 245]\n",
      "  [160 213 245]\n",
      "  [160 213 247]\n",
      "  ...\n",
      "  [165 160 138]\n",
      "  [155 150 128]\n",
      "  [138 133 111]]\n",
      "\n",
      " [[160 213 245]\n",
      "  [160 213 245]\n",
      "  [160 213 247]\n",
      "  ...\n",
      "  [162 157 135]\n",
      "  [155 150 128]\n",
      "  [145 140 118]]\n",
      "\n",
      " [[160 213 245]\n",
      "  [160 213 245]\n",
      "  [160 213 247]\n",
      "  ...\n",
      "  [154 149 127]\n",
      "  [151 146 124]\n",
      "  [144 139 119]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[174 192 202]\n",
      "  [174 192 202]\n",
      "  [174 192 202]\n",
      "  ...\n",
      "  [120 122 101]\n",
      "  [117 119  98]\n",
      "  [119 121  99]]\n",
      "\n",
      " [[175 193 203]\n",
      "  [176 194 204]\n",
      "  [176 194 204]\n",
      "  ...\n",
      "  [122 124 103]\n",
      "  [120 122 100]\n",
      "  [118 120  98]]\n",
      "\n",
      " [[175 193 203]\n",
      "  [176 194 204]\n",
      "  [175 193 203]\n",
      "  ...\n",
      "  [119 121  99]\n",
      "  [119 121  99]\n",
      "  [113 115  91]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_6.jpg - local variable 'shape' referenced before assignment\n",
      "[[[ 98  53  32]\n",
      "  [101  56  35]\n",
      "  [ 98  53  32]\n",
      "  ...\n",
      "  [ 40  36  35]\n",
      "  [ 46  42  41]\n",
      "  [ 50  46  45]]\n",
      "\n",
      " [[101  56  35]\n",
      "  [102  57  36]\n",
      "  [100  55  34]\n",
      "  ...\n",
      "  [ 47  43  42]\n",
      "  [ 43  39  38]\n",
      "  [ 45  41  40]]\n",
      "\n",
      " [[102  57  36]\n",
      "  [102  57  36]\n",
      "  [101  56  35]\n",
      "  ...\n",
      "  [ 53  49  48]\n",
      "  [ 43  39  38]\n",
      "  [ 44  40  39]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[175 155 122]\n",
      "  [176 156 123]\n",
      "  [174 154 121]\n",
      "  ...\n",
      "  [ 90  91  95]\n",
      "  [ 92  93  97]\n",
      "  [ 93  94  98]]\n",
      "\n",
      " [[176 156 123]\n",
      "  [178 158 125]\n",
      "  [176 156 123]\n",
      "  ...\n",
      "  [ 89  90  94]\n",
      "  [ 90  91  95]\n",
      "  [ 91  92  96]]\n",
      "\n",
      " [[179 159 126]\n",
      "  [180 160 127]\n",
      "  [179 159 126]\n",
      "  ...\n",
      "  [ 87  88  92]\n",
      "  [ 87  88  92]\n",
      "  [ 87  88  92]]]\n",
      "Number of faces detected: 0\n",
      "Failed on image: ./src/dataset/ppldb/people/vid/ppldb_vid_10.jpg - local variable 'shape' referenced before assignment\n",
      "\tDone!\n",
      "Mischief managed in 32.91198468208313s\n"
     ]
    }
   ],
   "source": [
    "# python src/utils/align_faces_parallel.py --root_path ./src/dataset/ppldb/vid/\n",
    "for person in glob(f\"./src/dataset/ppldb/people/*\"):\n",
    "    person_name = person.split(\"/\")[-1]\n",
    "    for i, image in enumerate(glob(f\"{person}/*\")):\n",
    "        if (image.split(\".\")[-1].lower() == \"jpg\" or image.split(\".\")[-1].lower() == \"png\") and not fnmatch(image.split(\"/\")[-1], f\"ppldb_{person_name}_*.jpg\"): \n",
    "            max_id = ppldb.get_max_iid(person_name)\n",
    "            os.rename(image, f\"{person}/ppldb_{person_name}_{max_id + 1}.jpg\")\n",
    "    os.system(f\"python src/utils/align_faces_parallel.py --root_path {person}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[3]:\n",
    "Project the aligned images into the latent space of StyleGAN2 and save the latent vectors in a new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/submodules/stylegan2-ada-pytorch\n",
      "./src/.tmp/stylegan2-ada-pytorch/child_to_parent024\n",
      "Got StyleGAN2 docker client, building image...\n",
      "StyleGAN2 Docker image built.\n",
      "[[], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "stylegan = StyleGAN2(tmp_path=output_path)\n",
    "people = []\n",
    "for person in glob(f\"./src/dataset/ppldb/people/*\"):\n",
    "  if os.path.isdir(person) and os.path.exists(f\"{person}/aligned\"):\n",
    "    people.append(stylegan.project_person(person))\n",
    "\n",
    "print(people)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In[4]:\n",
    "Migrate people from the ppldb dataset to nokdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nokdb_sample_filename = \"nokdb-samples-real-c2p\"\n",
    "for person in glob(f\"./src/dataset/ppldb/people/*\"):\n",
    "    person_name = person.split(\"/\")[-1]\n",
    "    people_names = nokdb.people_names()\n",
    "    if person_name not in people_names and os.path.exists(f\"{person}/latents\"):\n",
    "        pid = nokdb.max_nokdb_pid() + 1\n",
    "        pid\n",
    "        os.mkdir(f\"./src/dataset/nokdb/{pid}\")\n",
    "        # person = pid,name,family_name,sex,father_pid,mother_pid,race,sex_code,race_code\n",
    "        person_row = [pid, person.split(\"/\")[-1], \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "        nokdb.add_person(person_row)\n",
    "        for latent_folder in glob(f\"{person}/latents/*\"):\n",
    "            if not os.path.isdir(latent_folder):\n",
    "                continue\n",
    "            target_img = f\"{latent_folder}/target.png\"\n",
    "            latent = f\"{latent_folder}/projected_w.npz\"\n",
    "            iid = nokdb.max_nokdb_iid() + 1\n",
    "            shutil.copy(target_img, f\"./src/dataset/nokdb/{pid}/{iid}.png\")\n",
    "            shutil.copy(latent, f\"./src/dataset/nokdb/{pid}/{iid}.npz\")\n",
    "            # image = iid,pid,age,emotion,emotion_code\n",
    "            image_row = [iid, pid, \"\", \"\", \"\"]\n",
    "            nokdb.add_image(image_row)\n",
    "            sample_row = [\"\", \"\", \"\", \"\", pid, iid]\n",
    "            nokdb.add_sample(nokdb_sample_filename, sample_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(split):\n",
    "    X = []\n",
    "    y = []\n",
    "    for father, mother, child, child_gender, f_pid, m_pid, c_pid in NokMeanDataset(split=split):\n",
    "        input = child.flatten(0)\n",
    "        output = torch.cat([father.flatten(0), mother.flatten(0)], dim=0)\n",
    "        X.append(input)\n",
    "        y.append(output)\n",
    "    X = torch.stack(X, dim=0)\n",
    "    y = torch.stack(y, dim=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_real(split):\n",
    "    X = []\n",
    "    for father, mother, child, child_gender, f_pid, m_pid, c_pid in NokMeanDataset(split=split):\n",
    "        input = child.flatten(0)\n",
    "        X.append(input)\n",
    "    X = torch.stack(X, dim=0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 433 persons with 2676 images.\n",
      "Average images per person: 6.180138568129331\n",
      "Max images per person: 37\n",
      "Min images per person: 1\n",
      "[13, 12, 6, 1, 4, 2, 6, 9, 8, 2, 4, 7, 1, 6, 3, 3, 5, 8, 12, 4, 3, 6, 20, 4, 10, 3, 7, 3, 1, 4, 5, 9, 5, 8, 2, 4, 4, 3, 3, 2, 11, 2, 7, 14, 8, 10, 2, 1, 2, 9, 11, 5, 1, 1, 5, 2, 5, 5, 8, 1, 1, 5, 5, 3, 1, 3, 7, 23, 14, 15, 16, 5, 4, 1, 3, 9, 5, 6, 5, 1, 2, 5, 16, 4, 2, 1, 5, 2, 6, 24, 5, 15, 3, 9, 13, 2, 5, 6, 9, 7, 4, 8, 4, 7, 5, 11, 4, 9, 7, 6, 6, 7, 8, 5, 7, 5, 18, 10, 6, 8, 1, 5, 5, 9, 14, 6, 6, 1, 11, 14, 2, 7, 1, 2, 4, 4, 17, 5, 1, 6, 6, 4, 2, 3, 4, 5, 1, 2, 4, 4, 9, 3, 6, 2, 3, 5, 8, 6, 7, 14, 6, 5, 18, 9, 5, 3, 13, 3, 12, 5, 14, 5, 5, 1, 5, 9, 2, 9, 2, 6, 10, 8, 10, 12, 9, 6, 19, 10, 7, 37, 9, 6, 1, 1, 4, 24, 7, 3, 3, 4, 6, 13, 5, 8, 4, 5, 4, 2, 24, 5, 19, 6, 5, 13, 5, 3, 7, 8, 11, 4, 12, 7, 8, 7, 14, 15, 6, 8, 4, 5, 3, 5, 6, 8, 11, 6, 2, 2, 10, 4, 4, 3, 1, 1, 1, 1, 2, 1, 2, 5, 1, 16, 9, 8, 4, 4, 1, 9, 3, 5, 7, 8, 3, 2, 6, 4, 1, 5, 2, 4, 5, 6, 4, 2, 5, 5, 2, 7, 11, 13, 10, 7, 2, 5, 2, 4, 2, 4, 4, 1, 3, 1, 4, 3, 4, 4, 4, 3, 2, 7, 2, 11, 5, 4, 3, 1, 5, 11, 8, 3, 1, 9, 4, 4, 5, 7, 9, 6, 4, 2, 4, 16, 7, 4, 7, 8, 8, 9, 9, 3, 6, 10, 2, 6, 3, 5, 7, 2, 12, 5, 7, 6, 7, 20, 4, 3, 2, 1, 1, 2, 6, 4, 2, 5, 5, 3, 4, 6, 2, 24, 10, 16, 1, 5, 1, 5, 15, 14, 5, 3, 1, 3, 5, 2, 3, 16, 2, 1, 4, 2, 2, 5, 4, 3, 5, 10, 11, 1, 2, 3, 3, 1, 3, 9, 5, 4, 6, 12, 16, 4, 2, 12, 3, 3, 5, 4, 1, 7, 17, 1, 4, 4, 8, 7, 6, 5, 13, 12, 3, 5, 6, 6, 12, 9, 10, 5, 8, 8, 10, 16, 15, 13, 7]\n",
      "Loaded 60 persons with 508 images.\n",
      "Average images per person: 8.466666666666667\n",
      "Max images per person: 15\n",
      "Min images per person: 2\n",
      "[7, 9, 12, 7, 6, 11, 8, 7, 6, 12, 10, 8, 10, 9, 11, 10, 5, 8, 4, 5, 5, 7, 15, 4, 13, 5, 2, 12, 4, 12, 7, 10, 8, 15, 11, 4, 5, 6, 5, 4, 10, 10, 10, 9, 12, 6, 11, 10, 13, 9, 8, 10, 5, 8, 9, 7, 14, 8, 5, 15]\n",
      "Loaded 4 persons with 37 images.\n",
      "Average images per person: 9.25\n",
      "Max images per person: 23\n",
      "Min images per person: 4\n",
      "[6, 4, 23, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([207, 9216]),\n",
       " torch.Size([207, 18432]),\n",
       " torch.Size([20, 9216]),\n",
       " torch.Size([20, 18432]),\n",
       " torch.Size([4, 9216]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = load_data(split=\"train\")\n",
    "X_test, y_test = load_data(split=\"test\")\n",
    "X_real = load_data_real(split=\"real-c2p\")\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = Ridge()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.26190375e+00,  1.74879961e+00, -3.20254945e-01, ...,\n",
       "         -6.79779734e-01,  2.31499641e-01,  2.50138213e-01],\n",
       "        [-1.28690236e-01, -6.92713091e-01,  2.20155607e-02, ...,\n",
       "         -9.35918596e-01, -4.33072033e-01, -1.24392574e-01],\n",
       "        [ 9.30546944e-01,  1.06024714e+00,  1.10451361e+00, ...,\n",
       "         -8.80755055e-01,  1.44034903e-03,  3.07381018e-01],\n",
       "        ...,\n",
       "        [-3.95641814e-01, -3.83039171e-01,  1.28569897e-01, ...,\n",
       "         -6.12853036e-01, -4.60052297e-01, -2.11176730e-01],\n",
       "        [-2.53464931e-01, -8.02925866e-01,  1.51595021e+00, ...,\n",
       "          7.64996898e-02, -3.79286246e-01,  1.19212463e+00],\n",
       "        [ 3.30839508e-01,  6.18738526e-01,  1.40753507e+00, ...,\n",
       "         -3.52136868e-01, -7.66062544e-01, -1.55873831e-01]]),\n",
       " array([[ 1.14841027, -1.09105549,  1.01298357, ..., -0.6298971 ,\n",
       "         -0.50626142, -1.23926963],\n",
       "        [ 1.94331383, -0.95147925,  4.07622355, ..., -0.28963806,\n",
       "          0.91499323, -0.88502223],\n",
       "        [ 1.19397646,  0.75828222, -0.01601024, ..., -0.81827907,\n",
       "         -0.89090506,  0.4528127 ],\n",
       "        [ 1.39288519, -0.12895239, -1.97012552, ..., -1.22037777,\n",
       "         -0.03762387, -0.31539806]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hat = regressor.predict(X_train)\n",
    "y_test_hat = regressor.predict(X_test)\n",
    "y_real_hat = regressor.predict(X_real)\n",
    "y_test_hat, y_real_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006931318833174911, 1.0244137867456073)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_train = mean_squared_error(y_train, y_train_hat)\n",
    "mse_test = mean_squared_error(y_test, y_test_hat)\n",
    "mse_train, mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(y_real_hat.shape[0]):\n",
    "    images.append(X_real[i].view(18, 512))\n",
    "    images.append(torch.from_numpy(y_real_hat[i,:18*512]).to(torch.float32).view(18, 512))\n",
    "    images.append(torch.from_numpy(y_real_hat[i,18*512:]).to(torch.float32).view(18, 512))\n",
    "images = torch.stack(images, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    images.append(X_test[i].view(18, 512))\n",
    "    images.append(y_test[i,:18*512].view(18, 512))\n",
    "    images.append(y_test[i,18*512:].view(18, 512))\n",
    "    images.append(torch.from_numpy(y_test_hat[i,:18*512]).to(torch.float32).view(18, 512))\n",
    "    images.append(torch.from_numpy(y_test_hat[i,18*512:]).to(torch.float32).view(18, 512))\n",
    "images = torch.stack(images, dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./src/submodules/stylegan2-ada-pytorch\n",
      "./src/.tmp/stylegan2-ada-pytorch/child_to_parent024\n",
      "Got StyleGAN2 docker client, building image...\n",
      "StyleGAN2 Docker image built.\n"
     ]
    }
   ],
   "source": [
    "toTensor = torchvision.transforms.PILToTensor()\n",
    "toPIL = torchvision.transforms.ToPILImage()\n",
    "eval = BaseEvaluator()\n",
    "stylegan = StyleGAN2(tmp_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100 images from array...\n",
      "[INFO] StyleGAN2 - Generating image...\n",
      "['python3', './src/submodules/stylegan2-ada-pytorch/generate.py', '--network=https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl', '--outdir=./src/.tmp/stylegan2-ada-pytorch/child_to_parent024', '--noise-mode=random', '--projected-w=./src/.tmp/stylegan2-ada-pytorch/child_to_parent024/projected_w.npz']\n",
      "Loading networks from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"...\n",
      "\n",
      "Generating images from projected W \"./src/.tmp/stylegan2-ada-pytorch/child_to_parent024/projected_w.npz\"\n",
      "\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      "\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = 5\n",
    "pils = stylegan.generate_from_array(images.detach().cpu().numpy())\n",
    "pil = toPIL(torchvision.utils.make_grid([toTensor(pil.resize((128,128))) for pil in pils], nrow=rows)).convert(\"RGB\")\n",
    "pil.save(output_path + \"/result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pils_reshaped = np.array([np.array(pil).reshape(1024, 1024, 3) for pil in pils])\n",
    "pils_reshaped = pils_reshaped.reshape(-1, 5, 1024, 1024, 3)\n",
    "images_eval_f = pils_reshaped[:, 1]\n",
    "images_eval_m = pils_reshaped[:, 2]\n",
    "images_hat_eval_f = pils_reshaped[:, 3]\n",
    "images_hat_eval_m = pils_reshaped[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_eval_arr_f = [Image.fromarray(i).convert(\"RGB\") for i in images_eval_f]\n",
    "images_eval_arr_m = [Image.fromarray(i).convert(\"RGB\") for i in images_eval_m]\n",
    "images_hat_eval_arr_f = [Image.fromarray(i).convert(\"RGB\") for i in images_hat_eval_f]\n",
    "images_hat_eval_arr_m = [Image.fromarray(i).convert(\"RGB\") for i in images_hat_eval_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_eval_pil_f = toPIL(torchvision.utils.make_grid([toTensor(pil.resize((128, 128))) for pil in images_eval_arr_f], nrow=1)).convert(\"RGB\")\n",
    "images_eval_pil_f.save(output_path + \"/images_eval_f.png\")\n",
    "\n",
    "images_eval_pil_m = toPIL(torchvision.utils.make_grid([toTensor(pil.resize((128, 128))) for pil in images_eval_arr_m], nrow=1)).convert(\"RGB\")\n",
    "images_eval_pil_m.save(output_path + \"/images_eval_m.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x7ff624074970>\n",
      "<keras.engine.functional.Functional object at 0x7ff624074970>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17032194125479058,\n",
       " 0.42410156430016066,\n",
       " 0.5766179667264957,\n",
       " 0.20293526390898362,\n",
       " 0.6114542351559892,\n",
       " 0.20870337155936178,\n",
       " 0.20263176327657414,\n",
       " 0.20971202991459278,\n",
       " 0.45850289622454843,\n",
       " 0.2090265367134137,\n",
       " 0.3790095456356371,\n",
       " -0.10019846708355032,\n",
       " 0.3415420720789977,\n",
       " 0.1701254914614429,\n",
       " 0.4987418219148699,\n",
       " 0.17699413564308167,\n",
       " 0.4858469641219211,\n",
       " 0.06916089089432355,\n",
       " 0.2952165082434357,\n",
       " 0.0001356125163213019]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res_f_fn = eval.evaluate_batch(images_eval_f, images_hat_eval_f, model_name='Facenet512')\n",
    "images_hat_eval_arr_f_labeled_fn = zip(images_hat_eval_arr_f, eval_res_f_fn)\n",
    "images_hat_eval_pil_f_fn = toPIL(torchvision.utils.make_grid([toTensor(image_add_label(pil, str(round(label, 3)), 40).resize((256, 256))) for pil, label in images_hat_eval_arr_f_labeled_fn], nrow=1)).convert(\"RGB\")\n",
    "images_hat_eval_pil_f_fn.save(output_path + \"/images_eval_hat_f_fn.png\")\n",
    "eval_res_f_fn\n",
    "\n",
    "eval_res_m_fn = eval.evaluate_batch(images_eval_m, images_hat_eval_m, model_name='Facenet512')\n",
    "images_hat_eval_arr_m_labeled_fn = zip(images_hat_eval_arr_m, eval_res_m_fn)\n",
    "images_hat_eval_pil_m_fn = toPIL(torchvision.utils.make_grid([toTensor(image_add_label(pil, str(round(label, 3)), 40).resize((256, 256))) for pil, label in images_hat_eval_arr_m_labeled_fn], nrow=1)).convert(\"RGB\")\n",
    "images_hat_eval_pil_m_fn.save(output_path + \"/images_eval_hat_m_fn.png\")\n",
    "eval_res_m_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res_f_af = eval.evaluate_batch(images_eval_f, images_hat_eval_f, model_name='ArcFace')\n",
    "images_hat_eval_arr_f_labeled_af = zip(images_hat_eval_arr_f, eval_res_f_af)\n",
    "images_hat_eval_pil_f_af = toPIL(torchvision.utils.make_grid([toTensor(image_add_label(pil, str(round(label, 3)), 40).resize((256, 256))) for pil, label in images_hat_eval_arr_f_labeled_af], nrow=1)).convert(\"RGB\")\n",
    "images_hat_eval_pil_f_af.save(output_path + \"/images_eval_hat_f_af.png\")\n",
    "eval_res_f_af\n",
    "\n",
    "eval_res_m_af = eval.evaluate_batch(images_eval_m, images_hat_eval_m, model_name='ArcFace')\n",
    "images_hat_eval_arr_m_labeled_af = zip(images_hat_eval_arr_m, eval_res_m_af)\n",
    "images_hat_eval_pil_m_af = toPIL(torchvision.utils.make_grid([toTensor(image_add_label(pil, str(round(label, 3)), 40).resize((256, 256))) for pil, label in images_hat_eval_arr_m_labeled_af], nrow=1)).convert(\"RGB\")\n",
    "images_hat_eval_pil_m_af.save(output_path + \"/images_eval_hat_m_af.png\")\n",
    "eval_res_m_af\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res_vgg = eval.evaluate_batch(images_eval_f, images_hat_eval_f, model_name='VGG-Face')\n",
    "images_hat_eval_arr_f_labeled_vgg = zip(images_hat_eval_arr_f, eval_res_vgg)\n",
    "images_hat_eval_pil_f_vgg = toPIL(torchvision.utils.make_grid([toTensor(image_add_label(pil, str(round(label, 3)), 40).resize((256, 256))) for pil, label in images_hat_eval_arr_f_labeled_vgg], nrow=1)).convert(\"RGB\")\n",
    "images_hat_eval_pil_f_vgg.save(output_path + \"/images_eval_hat_f_vgg.png\")\n",
    "eval_res_vgg\n",
    "\n",
    "eval_res_vgg = eval.evaluate_batch(images_eval_m, images_hat_eval_m, model_name='VGG-Face')\n",
    "images_hat_eval_arr_m_labeled_vgg = zip(images_hat_eval_arr_m, eval_res_vgg)\n",
    "images_hat_eval_pil_m_vgg = toPIL(torchvision.utils.make_grid([toTensor(image_add_label(pil, str(round(label, 3)), 40).resize((256, 256))) for pil, label in images_hat_eval_arr_m_labeled_vgg], nrow=1)).convert(\"RGB\")\n",
    "images_hat_eval_pil_m_vgg.save(output_path + \"/images_eval_hat_m_vgg.png\")\n",
    "eval_res_vgg\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "2ca3a44b8209f7ad007e433b11a4146eceddfdad5cdb64f89eb05e06944554d2"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "5784ea47edf0f92e3a1ee20b3bb0ffe69aee936c1e03cfab6dae269c9ccd7409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
