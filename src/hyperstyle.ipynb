{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9cdff15abd20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor2im\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0minference_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_inversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdomain_adaptation_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_domain_adaptation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/fri-2022-diploma/src/submodules/hyperstyle/utils/domain_adaptation_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_inversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrestyle_inference_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "root = os.getenv('ROOT')\n",
    "sys.path.append(f\"{root}/submodules/hyperstyle/utils\")\n",
    "sys.path.append(f\"{root}/submodules/hyperstyle/notebooks\")\n",
    "\n",
    "\n",
    "from notebook_utils import run_alignment\n",
    "from common import tensor2im\n",
    "from inference_utils import run_inversion\n",
    "from domain_adaptation_utils import run_domain_adaptation\n",
    "from model_utils import load_model, load_generator\n",
    "\n",
    "\n",
    "\n",
    "#@title Select which domain you wish to perform inference on: { display-mode: \"form\" }\n",
    "experiment_type = 'faces' #@param ['faces', 'cars', 'afhq_wild']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"faces\": {\n",
    "        \"model_path\": \"./pretrained_models/hyperstyle_ffhq.pt\",\n",
    "        \"w_encoder_path\": \"./pretrained_models/faces_w_encoder.pt\",\n",
    "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"cars\": {\n",
    "        \"model_path\": \"./pretrained_models/hyperstyle_cars.pt\",\n",
    "        \"w_encoder_path\": \"./pretrained_models/cars_w_encoder.pt\",\n",
    "        \"image_path\": \"./notebooks/images/car_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((192, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"afhq_wild\": {\n",
    "        \"model_path\": \"./pretrained_models/hyperstyle_afhq_wild.pt\",\n",
    "        \"w_encoder_path\": \"./pretrained_models/afhq_wild_w_encoder.pt\",\n",
    "        \"image_path\": \"./notebooks/images/afhq_wild_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load HyperStyle Model { display-mode: \"form\" } \n",
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_model(model_path, update_opts={\"w_encoder_checkpoint_path\": EXPERIMENT_ARGS['w_encoder_path']})\n",
    "print('Model successfully loaded!')\n",
    "pprint.pprint(vars(opts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "if experiment_type == 'cars':\n",
    "    original_image = original_image.resize((192, 256))\n",
    "else:\n",
    "    original_image = original_image.resize((256, 256))\n",
    "original_image\n",
    "\n",
    "\n",
    "#@title Align Image (If Needed)\n",
    "input_is_aligned = False #@param {type:\"boolean\"}\n",
    "if experiment_type == \"faces\" and not input_is_aligned:\n",
    "    input_image = run_alignment(image_path)\n",
    "else:\n",
    "    input_image = original_image\n",
    "\n",
    "input_image.resize((256, 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" } \n",
    "n_iters_per_batch = 5 #@param {type:\"integer\"}\n",
    "opts.n_iters_per_batch = n_iters_per_batch\n",
    "opts.resize_outputs = False  # generate outputs at full resolution\n",
    "\n",
    "\n",
    "#@title Run Inference! { display-mode: \"form\" }\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(input_image) \n",
    "\n",
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_batch, result_latents, _ = run_inversion(transformed_image.unsqueeze(0).cuda(), \n",
    "                                                    net, \n",
    "                                                    opts,\n",
    "                                                    return_intermediate_results=True)\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opts.dataset_type == \"cars\":\n",
    "    resize_amount = (256, 192) if opts.resize_outputs else (512, 384)\n",
    "else:\n",
    "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)\n",
    "\n",
    "res = get_coupled_results(result_batch, transformed_image)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image \n",
    "outputs_path = \"./outputs\"\n",
    "os.makedirs(outputs_path, exist_ok=True)\n",
    "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5784ea47edf0f92e3a1ee20b3bb0ffe69aee936c1e03cfab6dae269c9ccd7409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
